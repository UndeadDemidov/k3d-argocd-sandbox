apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: vector
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: {{ .Values.project }}
  destination:
    name: "{{ .Values.cluster }}"
    namespace: {{ .Values.ns }}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
  sources:
  - repoURL: https://helm.vector.dev
    chart: vector
    targetRevision: 0.50.0
    helm:
      valuesObject:
        role: Agent
        dataDir: /vector-data-dir
        resources: {}
        args:
          - -w
          - --config-dir
          - /etc/vector/
        containerPorts:
          - name: metrics
            containerPort: 9090
            protocol: TCP
        service:
          enabled: false
        customConfig:
          data_dir: /vector-data-dir
          api:
            enabled: false
            address: 0.0.0.0:8686
            playground: true
          sources:
            k8s:
              type: kubernetes_logs
          transforms:
            # 1. Нормализация: вытаскиваем k8s-поля, оставляем .message
            normalize:
              type: remap
              inputs: [k8s]
              source: |-
                .image = .kubernetes.container_image
                .container = .kubernetes.container_name
                .pod = .kubernetes.pod_name
                .ip = .kubernetes.pod_ip
                .namespace = .kubernetes.pod_namespace
                .node = .kubernetes.pod_node_name
                del(.file)
                del(.kubernetes)

            # 2. Детектор типа: json / logfmt / plain (по порядку, первый подходящий)
            # JSON: успешный parse_json и результат — объект или массив
            # logfmt: есть хотя бы одна пара key=value (регулярка), иначе plain текст дал бы ключи со значением true
            detect:
              type: remap
              inputs: [normalize]
              source: |-
                parsed, err = parse_json(.message)
                json_ok = err == null && (starts_with(encode_json(parsed), "{") || starts_with(encode_json(parsed), "["))
                logfmt_ok = match(.message, r'[^\s=]+=(?:"[^"]*"|\S+)') ?? false
                .log_format = if json_ok { "json" } else if logfmt_ok { "logfmt" } else { "plain" }

            # 3. Маршрутизация по типу (первый совпавший забирает событие)
            route_by_type:
              type: exclusive_route
              inputs: [detect]
              routes:
                - name: json
                  condition:
                    type: vrl
                    source: '.log_format == "json"'
                - name: logfmt
                  condition:
                    type: vrl
                    source: '.log_format == "logfmt"'

            # 3b. Plain: объединяем многострочные Java stack trace в одно событие (reduce)
            # Новое событие — когда строка НЕ продолжение (не "\t", не "  at ", не "Caused by:", не "Suppressed:")
            merge_plain_multiline:
              type: reduce
              inputs: [route_by_type._unmatched]
              group_by: [namespace, pod, container]
              merge_strategies:
                message: concat_newline
              starts_when:
                type: vrl
                source: |-
                  !(match(string!(.message), r'^\t') || match(string!(.message), r'^\s+at\s') || match(string!(.message), r'^Caused by:') || match(string!(.message), r'^Suppressed:'))
              expire_after_ms: 3000
              flush_period_ms: 500

            # 4a. Ветка JSON: парсим и собираем .log / .msg; при необходимости парсим вложенный JSON в .message
            parse_json:
              type: remap
              inputs: [route_by_type.json]
              source: |-
                .log = parse_json!(.message)
                if exists(.log."@version") {
                  .log.version = .log."@version"
                  del(.log."@version")
                  if exists(.log."@timestamp") {
                    .log.timestamp = .log."@timestamp"
                    del(.log."@timestamp")
                  }
                  if exists(.log."app-name") {
                    .log.app_name = .log."app-name"
                    del(.log."app-name")
                  }
                }
                .msg = encode_json(.log)
                if exists(.log.level) && .log.level != null && .log.level != "" { .level = .log.level }
                if exists(.log.level_value) {
                  v, err = to_string(.log.level_value)
                  if err == null {
                    .level = if v == "20000" { "info" } else if v == "10000" { "debug" } else if v == "30000" { "warn" } else if v == "40000" { "error" } else if v == "50000" { "fatal" } else if v == "5000" { "trace" } else { .level }
                  }
                }
                del(.message)

            # 4b. Ветка logfmt: parse_logfmt (logfmt key=value), потом .log / .msg
            parse_logfmt:
              type: remap
              inputs: [route_by_type.logfmt]
              source: |-
                .log = parse_logfmt!(.message)
                if exists(.log.msg) {
                  .log.message = .log.msg
                  del(.log.msg)
                }
                .msg = encode_json(.log)
                if exists(.log.level) && .log.level != null && .log.level != "" { .level = .log.level }
                del(.message)

            # 4c. Ветка plain: парсинг Java/Log4j (DD-Mon-YYYY HH:MM:SS.mmm LEVEL [thread] logger message),
            #     Kafka producer ([thread] LEVEL logger - message) или .msg в одну строку
            parse_plain:
              type: remap
              inputs: [merge_plain_multiline]
              source: |-
                # Если лог содержит ".java:<число>)", то это stack trace — парсить не надо
                if match(.message, r'\.java:\d+\)') ?? false {
                  .level = "error"
                  .msg = .message
                  del(.message)
                } else {
                  .level = "unknown"
                  # Java/Log4j: 05-Feb-2026 14:46:25.841 INFO [pool-4-thread-7] logger.name message text
                  # Kafka: [kafka-producer-network-thread | producer-1] INFO com.farzoom.service.impl.KafkaMessageCallback - Message with UUID ... was send to topic ...
                  # Logback internal: 11:56:39,151 |-INFO in ch.qos.logback...@21c7208d - message (tag опционально в regex)
                  .log = parse_regex(.message, r'^(?P<timestamp>\d{2}-\w{3}-\d{4}\s+\d{2}:\d{2}:\d{2}\.\d{3})\s+(?P<level>\w+)\s+\[(?P<thread>[^\]]+)\]\s+(?P<logger_name>[a-zA-Z0-9_.]+)\s+(?P<message>.*)$') ??
                        parse_regex(.message, r'^\[(?P<thread>[^\]]+)\]\s+(?P<level>\w+)\s+(?P<logger_name>[a-zA-Z0-9_.]+)\s+-\s+(?P<message>.*)$') ??
                        parse_regex(.message, r'^(?P<timestamp>\d{1,2}:\d{2}:\d{2},\d{3})\s+\|\-(?P<level>\w+)\s+in\s+(?P<logger_name>[^\s]+?)(?:@(?P<tag>[0-9a-fA-F]+))?\s+-\s+(?P<message>.*)$') ??
                        null
                  if exists(.log) && exists(.log.message) {
                    .level = downcase!(.log.level)
                    .msg = encode_json(.log)
                  } else {
                    .msg = .message
                    del(.message)
                  }
                }

          sinks:
            vlogs:
              type: elasticsearch
              inputs: [parse_json, parse_logfmt, parse_plain]
              endpoints:
                - http://vl-single-server.{{ .Values.ns }}.svc:9428/insert/elasticsearch/
              mode: bulk
              api_version: v8
              compression: gzip
              healthcheck:
                enabled: false
              request:
                headers:
                  VL-Time-Field: timestamp
                  VL-Stream-Fields: stream,node,namespace,pod,container
                  VL-Msg-Field: msg
                  AccountID: "0"
                  ProjectID: "0"